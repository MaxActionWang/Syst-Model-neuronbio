{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMDA homework 1, question 2 Parkinson\n",
    "王敏行 ID:2018012386 wangmx18@mails.tsinghua.edu.cn\n",
    "\n",
    "Following codes are generated with the help of official documents and books.\n",
    "\n",
    "## Part1: Logistic Regression on Parkinson dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)',\n",
      "       'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP',\n",
      "       'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',\n",
      "       'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'spread1',\n",
      "       'spread2', 'D2', 'PPE'],\n",
      "      dtype='object')\n",
      "data shape: (195, 22); no. positive: 147; no. negative: 48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pa = pd.read_table('parkinsons.data',sep=\",\")\n",
    "\n",
    "x = pa.drop(['name','status'], axis=1)\n",
    "X = np.array(x)\n",
    "features = x.columns\n",
    "print(features)\n",
    "# X = np.array(pa.iloc[0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1])\n",
    "#\"status\" 0 for healthy and 1 for PD\n",
    "y = pa.status\n",
    "print('data shape: {0}; no. positive: {1}; no. negative: {2}'.format(\n",
    "    X.shape, y[y==1].shape[0], y[y==0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR matchs: 39/39\n",
      "SVM matchs: 39/39\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(penalty='l1',solver='liblinear') #solver='liblinear', default solver lbfgs doesn't support l1 penalty\n",
    "LR.fit(X_train, y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print('LR matchs: {0}/{1}'.format(np.equal(y_pred, y_test).shape[0], y_test.shape[0]))\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print('SVM matchs: {0}/{1}'.format(np.equal(y_pred, y_test).shape[0], y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain more solid conclusion, we repeat the train-test split process several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is LogisticRegression(penalty='l1', solver='liblinear')\n",
      "train score: 0.852564; test score: 0.820513\n",
      "model is SVC(kernel='linear')\n",
      "train score: 0.871795; test score: 0.871795\n",
      "model is LogisticRegression(penalty='l1', solver='liblinear')\n",
      "train score: 0.833333; test score: 0.897436\n",
      "model is SVC(kernel='linear')\n",
      "train score: 0.871795; test score: 0.923077\n",
      "model is LogisticRegression(penalty='l1', solver='liblinear')\n",
      "train score: 0.871795; test score: 0.820513\n",
      "model is SVC(kernel='linear')\n",
      "train score: 0.903846; test score: 0.846154\n",
      "model is LogisticRegression(penalty='l1', solver='liblinear')\n",
      "train score: 0.858974; test score: 0.820513\n",
      "model is SVC(kernel='linear')\n",
      "train score: 0.897436; test score: 0.820513\n",
      "model is LogisticRegression(penalty='l1', solver='liblinear')\n",
      "train score: 0.897436; test score: 0.743590\n",
      "model is SVC(kernel='linear')\n",
      "train score: 0.897436; test score: 0.820513\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    for model in [LR, svm]:\n",
    "        print('model is', model)\n",
    "        model.fit(X_train, y_train)\n",
    "        train_score = model.score(X_train, y_train)\n",
    "        test_score = model.score(X_test, y_test)\n",
    "        print('train score: {train_score:.6f}; test score: {test_score:.6f}'.format(\n",
    "            train_score=train_score, test_score=test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面重复了5次，从test accuracy的结果来看，线性SVM的表现更好。\n",
    "Logistic回归的目的是使得总loss最小，SVM的优化目的是使得margin最大。二者最终的目的都是使得代价函数$J(w)$最小。其中，Logistic回归采用p1正则项，其代价函数为\n",
    "$$\n",
    "J(w)=\\lambda \\sum|\\theta|+\\sum y_i log(h_{\\theta}(x_i))+(1-y_i) log(1-h_{\\theta}(x_i)))\n",
    "$$\n",
    "SVM的代价函数为\n",
    "$$\n",
    "J(w)=\\lambda \\sum\\theta^2+\\sum(h_{\\theta}(x_i)-y_i)^2\n",
    "$$\n",
    "说明这个问题情景下，采用SVM更合适。\n",
    "\n",
    "## Part 2: the most important feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.884615; test score: 0.846154 feature MDVP:Fo(Hz) 1\n",
      "train score: 0.884615; test score: 0.820513 feature MDVP:Fhi(Hz) 2\n",
      "train score: 0.858974; test score: 0.948718 feature MDVP:Flo(Hz) 3\n",
      "train score: 0.852564; test score: 0.897436 feature MDVP:Jitter(%) 4\n",
      "train score: 0.891026; test score: 0.871795 feature MDVP:Jitter(Abs) 5\n",
      "train score: 0.891026; test score: 0.897436 feature MDVP:RAP 6\n",
      "train score: 0.871795; test score: 0.846154 feature MDVP:PPQ 7\n",
      "train score: 0.923077; test score: 0.717949 feature Jitter:DDP 8\n",
      "train score: 0.865385; test score: 0.923077 feature MDVP:Shimmer 9\n",
      "train score: 0.884615; test score: 0.820513 feature MDVP:Shimmer(dB) 10\n",
      "train score: 0.878205; test score: 0.897436 feature Shimmer:APQ3 11\n",
      "train score: 0.891026; test score: 0.794872 feature Shimmer:APQ5 12\n",
      "train score: 0.884615; test score: 0.897436 feature MDVP:APQ 13\n",
      "train score: 0.865385; test score: 0.897436 feature Shimmer:DDA 14\n",
      "train score: 0.891026; test score: 0.820513 feature NHR 15\n",
      "train score: 0.878205; test score: 0.871795 feature HNR 16\n",
      "train score: 0.858974; test score: 0.897436 feature RPDE 17\n",
      "train score: 0.878205; test score: 0.846154 feature DFA 18\n",
      "train score: 0.871795; test score: 0.871795 feature spread1 19\n",
      "train score: 0.878205; test score: 0.897436 feature spread2 20\n",
      "train score: 0.897436; test score: 0.820513 feature D2 21\n",
      "train score: 0.865385; test score: 0.897436 feature PPE 22\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(features)\n",
    "for i in range(22):\n",
    "    XX = np.delete(X,i, axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size=0.2)\n",
    "    svm.fit(X_train, y_train)\n",
    "    train_score = svm.score(X_train, y_train)\n",
    "    test_score = svm.score(X_test, y_test)\n",
    "    print('train score: {train_score:.6f}; test score: {test_score:.6f}'.format(\n",
    "        train_score=train_score, test_score=test_score),'feature', feature_names[i], i+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过随机删除某一个特征，用剩下的21维特征进行训练并验证其测试集上准确度。则可以认为，test score较低的的数据集对应的去掉的特征是重要的。\n",
    "发现得到的结果与`train_test_split`中的参数`random_state`关系密切，波动较大。故可以认为，不存在对于分类贡献较大的单一特征。或者存在至少两个或以上可以用于分类的特征，从而去掉一个feature对于结果影响不大。\n",
    "方法改进：可以通过多次训练、打分、取平均值的方式减弱random_state的影响。还可以按照status和某一feature之间的信息熵变，计算出22个feature带来的信息熵变。变化最大的理应贡献最大。\n",
    "\n",
    "## Part 3: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best param: {'C': 0.5644444444444445, 'gamma': 0.001}\n",
      "best score: 0.7846153846153847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'gamma')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "range_gamma = np.linspace(0.001, 5, 20)\n",
    "range_c = np.linspace(0.01,5,10)\n",
    "param_grid = {'gamma': range_gamma,'C':range_c}\n",
    "clf = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, return_train_score=True)\n",
    "clf.fit(X, y)\n",
    "print(\"best param: {0}\\nbest score: {1}\".format(clf.best_params_,clf.best_score_))\n",
    "clf.best_estimator_\n",
    "\n",
    "best_point = list(clf.best_params_.values())\n",
    "C_best = best_point[0]\n",
    "gamma_best = best_point[1]\n",
    "plt.scatter(C_best,gamma_best)\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('gamma')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93cb51f5991ec0b90525fa9a9f56cc67747d2aebc05662b8a7d03c580c444d66"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
